# Why Axonize?

> AI 추론 인프라를 위한 옵저버빌리티 플랫폼

---

## 문제: AI 추론의 블랙박스

### GPU는 비싸다

```
H100 80GB: 시간당 $3-4 (클라우드)
8x H100 클러스터: 월 $70,000+
```

그런데 **실제로 얼마나 효율적으로 쓰고 있는지 모릅니다.**

---

### 현재 모니터링의 한계

```
┌─────────────────────────────────────────────────────────────────┐
│                     현재 상황                                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Grafana/Prometheus          Langfuse/LangSmith                │
│   ─────────────────          ──────────────────                 │
│   "GPU utilization 87%"      "API 호출 성공"                    │
│                                                                  │
│   → 근데 이 87% 중에서        → 근데 Self-hosted 추론은?        │
│     실제 추론은 몇 %?           vLLM, Diffusers는?              │
│     어떤 요청이 느린가?                                          │
│     GPU를 얼마나 쓴가?                                          │
│                                                                  │
│                        [ GAP ]                                   │
│                                                                  │
│   인프라 레벨               ←  ???  →           서비스 레벨      │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

### 구체적인 Pain Points

**ML Engineer:**
> "이미지 생성이 갑자기 느려졌는데, 어디가 병목인지 모르겠어요.
> VIT? Diffusion? VAE? 20개 step 중에 어디서 느린가요?"

**Platform Team:**
> "GPU 8장 쓰는데 utilization이 들쭉날쭉해요.
> 어떤 모델이 GPU를 비효율적으로 쓰는지 알 수 없어요."

**Finance:**
> "추론당 비용이 얼마인지 산정이 안 돼요.
> 고객별로 GPU 비용을 어떻게 청구하죠?"

---

## 해결: Axonize

### 핵심 가치

```
┌─────────────────────────────────────────────────────────────────┐
│                     Axonize가 제공하는 것                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. 추론 단계별 breakdown                                        │
│     ─────────────────────                                        │
│     "VIT: 120ms → Diffusion: 3200ms → VAE: 180ms"               │
│     "20 step 중 step 7-9가 평균보다 40% 느림"                    │
│                                                                  │
│  2. GPU 귀속                                                     │
│     ──────────                                                   │
│     "이 추론은 GPU-0,1,2,3을 썼고, 각각 utilization은..."       │
│     "MIG 인스턴스별 사용량 추적"                                 │
│                                                                  │
│  3. 비용 산정                                                    │
│     ──────────                                                   │
│     "이 추론의 GPU 비용: $0.0023"                                │
│     "이번 달 모델별 비용: SDXL $1,200 / LLaMA $800"             │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

### 시장 포지셔닝

```
┌─────────────────────────────────────────────────────────────────┐
│                                                                  │
│   인프라 모니터링          [Axonize]          LLM 서비스 추적    │
│   Grafana/Prometheus      ──────────         Langfuse/LangSmith │
│                                                                  │
│   "서버가 살아있나?"      "모델이 최적으로    "API 호출이        │
│                           돌아가나?"          성공했나?"         │
│                                                                  │
│   CPU, Memory             TTFT, TPOT,        Prompt, Token,     │
│   GPU Utilization         Step Latency,      Completion         │
│                           GPU Attribution                        │
│                                                                  │
│   범용                    AI 추론 특화        LLM API 특화       │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 왜 지금인가?

### 1. Self-hosted 추론의 폭발적 성장

```
2023: ChatGPT API로 충분
2024: vLLM, Ollama, TGI로 Self-hosted 전환
2025: 비용 절감 + 데이터 보안으로 필수화
```

- vLLM GitHub Stars: 35K+ (2024)
- Ollama 다운로드: 수백만
- 기업 60%+ Self-hosted 추론 검토 중

---

### 2. 멀티모달 AI의 확산

```
LLM만 있던 시절: Langfuse로 충분
이미지 생성 추가: ??? (표준 없음)
오디오/비디오 추가: ??? (표준 없음)
```

**Axonize는 모든 모달리티를 하나의 규격으로 통합**

---

### 3. GPU 비용의 현실화

```
스타트업 A: "우리 GPU 비용이 월 $50K인데, 어디에 쓰이는지 모름"
스타트업 B: "고객별로 GPU 비용 청구하고 싶은데 방법이 없음"
대기업 C: "GPU 클러스터 효율이 40%도 안 되는 것 같은데 증명할 수 없음"
```

---

## Target Users

| User | Use Case | Axonize Value |
|------|----------|---------------|
| **ML Engineer** | 추론 서버 운영/최적화 | 병목 구간 즉시 파악 |
| **Platform Team** | GPU 클러스터 관리 | GPU 효율성 수치화 |
| **Finance/Ops** | 비용 관리/청구 | 추론당 비용 산정 |

---

## 기술적 차별점

### 1. OpenTelemetry 호환
```
- 기존 Jaeger/Grafana와 통합 가능
- 도입 장벽 최소화
- vendor lock-in 없음
```

### 2. GPU 3-Layer Identity Model
```
Physical GPU → Compute Resource → Runtime Context

MIG 환경에서도 정확한 GPU 귀속
Pod 재시작해도 추적 유지
```

### 3. Zero-Overhead First
```
목표: 추론 성능에 1% 미만 영향
비동기 수집, 배치 전송
```

---

## 경쟁 분석

| 도구 | 강점 | Axonize 대비 약점 |
|------|------|------------------|
| **Langfuse** | LLM 트레이싱 | Self-hosted 추론 미지원, 이미지/오디오 없음 |
| **Weights & Biases** | 학습 추적 | 추론 최적화 기능 부족 |
| **Grafana** | 범용 모니터링 | 모델 내부 메트릭 불가 |
| **Jaeger** | 분산 트레이싱 | AI/GPU 특화 없음 |

---

## 비전

```
"모든 AI 추론의 모든 단계를, 모든 GPU에서 추적한다"

Phase 1: Self-hosted 추론 모니터링 (MVP)
Phase 2: 멀티 노드/분산 추론 지원
Phase 3: 자동 최적화 추천
Phase 4: AI 추론 인프라의 표준
```

---

## 요약

```
문제: AI 추론이 블랙박스. GPU 비용은 비싼데 효율성 측정 불가.

해결: Axonize - 추론 단계별 breakdown + GPU 귀속 + 비용 산정

왜 지금: Self-hosted 폭발 + 멀티모달 확산 + GPU 비용 현실화

차별점: OTel 호환 + GPU 3-Layer Identity + Zero-Overhead
```
